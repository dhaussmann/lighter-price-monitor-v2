# âœ… Setup Checklist - Lighter Orderbook Tracker

## ðŸ“¦ Dateien erstellt

### Core Implementation
- [x] `worker/src/aggregator-new.ts` - Streaming Aggregator (293 Zeilen)
- [x] `worker/src/lighter-new.ts` - Lighter Tracker DO (461 Zeilen)
- [x] `worker/src/worker-new.ts` - Main Worker + API + Frontend (515 Zeilen)

### Configuration & Schema
- [x] `worker/schema-new.sql` - Clean Database Schema
- [x] `worker/wrangler-new.toml` - Wrangler Config

### Documentation
- [x] `DEPLOYMENT-GUIDE.md` - VollstÃ¤ndiger Deployment-Guide
- [x] `SETUP-CHECKLIST.md` - Diese Datei

### Backup
- [x] `worker/backup/*.ts` - Backup der alten Files

---

## ðŸš€ Deployment Steps (Kopiere & FÃ¼hre aus)

```bash
# === Schritt 1: Database Setup ===
cd /home/user/lighter-price-monitor-v2/worker

# Reset alte Tabellen (optional)
wrangler d1 execute DB --remote --command "DROP TABLE IF EXISTS orderbook_entries"
wrangler d1 execute DB --remote --command "DROP TABLE IF EXISTS orderbook_snapshots"
wrangler d1 execute DB --remote --command "DROP TABLE IF EXISTS orderbook_minutes"
wrangler d1 execute DB --remote --command "DROP TABLE IF EXISTS paradex_trades"
wrangler d1 execute DB --remote --command "DROP TABLE IF EXISTS token_mapping"

# Neue Schema erstellen
wrangler d1 execute DB --remote --file=schema-new.sql

# Verify
wrangler d1 execute DB --remote --command "SELECT name FROM sqlite_master WHERE type='table'"

# === Schritt 2: Code Deployment ===

# Backup alte Config
cp wrangler.toml wrangler.toml.backup

# Aktiviere neue Config
cp wrangler-new.toml wrangler.toml

# Deploy!
npx wrangler deploy

# === Schritt 3: Test ===

# Ã–ffne Frontend
# URL: https://lighter-orderbook-tracker.YOUR-SUBDOMAIN.workers.dev

# Teste API
curl https://YOUR-WORKER.workers.dev/api/stats | jq
curl https://YOUR-WORKER.workers.dev/api/markets | jq

# Live Logs
npx wrangler tail
```

---

## ðŸ“Š Was ist NEU?

### Architektur
| Alt | Neu |
|-----|-----|
| Multi-DO (Lighter + Paradex) | Single DO (nur Lighter) |
| Direkte DB Inserts | Streaming Aggregation |
| ~1000+ Inserts/min | ~4 Snapshots/min |
| Memory wÃ¤chst | Memory konstant ~75KB |
| Komplexes Setup | Einfaches Setup |

### Market Mapping
**Alt:**
- API: `https://mainnet.zklighter.elliot.ai/api/v1/orderBooks`
- Problem: Komplexe Struktur, Extraktion nÃ¶tig

**Neu:**
- API: `https://explorer.elliot.ai/api/markets`
- Format: `[{symbol: "ETH", market_index: 1}, ...]`
- Clean & Simple!

### Datenfluss
```
Lighter WebSocket
  â†“
Process Orderbook (beste Bid/Ask)
  â†“
Aggregator.process(symbol, bid, ask)
  â†“
[Memory Window: 15s]
  â†“
Flush â†’ lighter_snapshots
  â†“
[4 Snapshots = 1 Minute]
  â†“
Calculate Average â†’ lighter_minutes
  â†“
Cleanup old snapshots
```

---

## ðŸŽ¯ Checkliste fÃ¼r Erfolgreiches Deployment

### Vor dem Deployment
- [ ] Datenbank-Reset durchgefÃ¼hrt
- [ ] Neue Schema erstellt
- [ ] Tabellen verifiziert

### Nach dem Deployment
- [ ] Frontend Ã¶ffnet ohne Fehler
- [ ] WebSocket verbindet (Check logs)
- [ ] Status zeigt "CONNECTING..." â†’ "STOPPED"
- [ ] Start Button funktioniert
- [ ] Nach Start: Status = "TRACKING"
- [ ] Messages Counter steigt
- [ ] Markets > 0
- [ ] Logs zeigen Activity

### Nach 1 Minute
- [ ] Snapshots in DB (>0)
- [ ] Minutes in DB (>0)
- [ ] API `/api/stats` funktioniert
- [ ] API `/api/markets` zeigt Markets
- [ ] API `/api/minutes` zeigt Daten

### Monitoring (kontinuierlich)
- [ ] Keine Memory-Limit Errors
- [ ] WebSocket bleibt connected
- [ ] Messages kommen kontinuierlich
- [ ] Aggregator flushed regelmÃ¤ÃŸig (alle 15s)
- [ ] Minuten-Aggregation lÃ¤uft (alle 60s)

---

## ðŸ› Troubleshooting

### "Keine Daten kommen"

**1. Check WebSocket:**
```bash
npx wrangler tail
# Suche: "[Lighter] âœ… WebSocket connected"
```

**2. Check Markets:**
```bash
wrangler d1 execute DB --remote --command "SELECT COUNT(*) FROM lighter_markets"
# Sollte > 0 sein
```

**3. Check Messages:**
```
Frontend: Messages Counter muss steigen
Logs: "[Lighter] Message received" sollte regelmÃ¤ÃŸig kommen
```

### "Memory Limit Error"

**Das sollte NICHT mehr passieren!**

Falls doch:
1. Check Aggregator Window Size
2. ErhÃ¶he `windowDuration` auf 30000 (30s)
3. Reduce Markets (Filter in loadMarkets())

### "Keine Minuten-Aggregation"

Check Logs fÃ¼r:
```
[Aggregator] ðŸ“Š Calculating minute average for...
[Aggregator] âœ… Aggregated XX minute averages
```

Falls fehlt:
- Window-Position Bug â†’ Check Code
- Snapshots fehlen â†’ Check Flush Logs

---

## ðŸ“ˆ Performance Metriken

### Memory
- **Target:** < 100 KB
- **Aktuell:** ~75 KB
- **Monitoring:** Check Cloudflare Dashboard

### Database Writes
- **Snapshots:** ~4 writes/min/symbol
- **Minutes:** 1 write/min/symbol
- **Bei 100 Symbolen:** ~400 writes/min (vs 10,000+ vorher!)

### WebSocket
- **Messages:** ~1000+/min (abhÃ¤ngig von Market Activity)
- **Ping:** Alle 30s
- **Reconnect:** Auto nach 5s bei Disconnect

---

## ðŸŽ‰ Success Criteria

Du weiÃŸt dass alles funktioniert wenn:

1. âœ… Frontend zeigt "ðŸŸ¢ TRACKING"
2. âœ… Markets Count > 100
3. âœ… Messages Counter steigt kontinuierlich
4. âœ… Last Message = "Xs ago" (< 5s)
5. âœ… Snapshots in DB wachsen
6. âœ… Minutes in DB wachsen
7. âœ… Logs zeigen Flush + Aggregation
8. âœ… API liefert sinnvolle Daten
9. âœ… Keine Errors in Logs
10. âœ… Memory bleibt konstant

---

## ðŸ“ž Next Steps

Nach erfolgreichem Deployment:

1. **Monitoring Setup**
   - Cloudflare Analytics Dashboard
   - Custom Alerts fÃ¼r Disconnects
   - Memory Usage Monitoring

2. **API Integration**
   - Baue Charts mit `/api/minutes` Daten
   - Historical Data Export
   - Real-time Updates via WebSocket

3. **Features erweitern**
   - Paradex hinzufÃ¼gen (spÃ¤ter)
   - Mehr Aggregation Levels (5min, 1h)
   - Trade Volume Tracking
   - Spread Analysis

4. **Production Hardening**
   - Rate Limiting
   - Error Handling verbessern
   - Backup Strategy
   - Disaster Recovery Plan

---

**Viel Erfolg! ðŸš€**

Bei Fragen â†’ Check `DEPLOYMENT-GUIDE.md`
